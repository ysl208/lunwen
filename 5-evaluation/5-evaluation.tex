\newpage
\section{Experiment 2: Acceptance of a Robot Programming Framework}

% \section{Can Users teach Action Models for Automated Planning using the Robot Programming Framework?}
\label{sec:Exp2}
%%%%%%%% 1.25 pages %%%%%%%

In this experiment, users were presented the robot programming framework (\chapt{chap:Contribution}), and had to teach action models by kinesthetic manipulation of a Baxter robot (Fig. \ref{fig:Baxter}). Users were instructed to demonstrate an atomic action and assign preconditions and effects. The goal was to assess the framework's usability and difficulties encountered, when teaching action models.

\subsection{Experimental Setup \& Participants}
We recruited 11 participants (7 male, 4 female), who were students and staff members at Universit\'{e} Grenoble Alpes. 6 participants reported programming experience with office productivity software (`beginner'), 2 had previously taken a programming course before (`advanced'), and 3 were pursuing studies in Computer Science (`expert').
%4 participants had previously heard of Automated Planning, but only 3 attended a related course.
The experiments were conducted using a Baxter robot (Fig. \ref{fig:Baxter}), mounted with a partial implementation of the framework (e.g. `learn a new action', `find an object', `execute an action sequence'). We used the Wizard-of-Oz technique to simulate the remaining functionalities (e.g. `generate action sequence').
Participants operated on a table with 2 positions (A and B), 2 cubes (blue and red), that represented parts on an assembly line. 
Each participant was allocated 1 hour, but the average duration was 29.5 minutes. The experiments were recorded, while the participant interacted with the robot. 


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/scenarios-exp2}
	\caption{Continuous refinement of the move action model: (a) initial action model learned by demonstration, (b) action model for all cubes of any colour, (c) action model with an additional condition, if the target position is occupied and cubes can not be stacked.}
	\label{fig:scenarios-exp2}
\end{figure} 
%The complete experimental protocol is shown in Fig. \ref{fig:Experimental protocol}. 
\subsection{Experimental Design \& Measurements}
The experiment was set in a simulated assembly line, where objects of the same shape, but different colour arrived consecutively at position A.
Users had to teach Baxter the action for moving an object from position A to position B, where objects should not be stacked. Throughout the experiment, users were faced with two different scenarios, where Baxter had to apply the learned action model. We evaluated the user's capability to refine action models, when faced with different situations, and wanted to assess the framework's overall usability.
The experiment consisted of the following phases:
\begin{itemize}
  %\item{Introduction: After a short introduction to the Baxter robot \cite{Baxter}, users were told that they needed to use a planning language (STRIPS) to explain Baxter the state of the world and the semantic meaning of the actions.}
  \item{\textbf{Training:} Users were shown how to manipulate Baxter's arm, and given time to familiarise themselves with the kinesthetic manipulation.}
  \item{\textbf{Experimental test:} Users were instructed to teach Baxter a move action of a cube. Then, they were presented the action model, with preconditions and effects, that Baxter learned from the demonstration (Fig. \ref{fig:scenarios-exp2}a). In the following, users were faced with two different scenarios, to refine the conditions of the action model. Starting with the initial action model for a red cube, users modified the conditions, so that it was applicable to all cubes of any colour (Fig. \ref{fig:scenarios-exp2}b), and when the target position was occupied (Fig. \ref{fig:scenarios-exp2}c). At each step, users observed how Baxter applied the learned action in the new scenario. When Baxter failed to execute the action, users had to refine the conditions of the action model.}
  \item{\textbf{Planning:} Users were presented a new scenario, where Baxter was instructed to achieve a new goal using the learned action model. The new goal was to switch the positions of two cubes on the table. Baxter demonstrated this by executing an action sequence generated by an automated planner.}
  \item{\textbf{Questionnaire:} Users were given a questionnaire containing 18 questions related to their experience (Fig. \ref{fig:eEvaluation}).}
   \item{ \textbf{Debriefing:} Users were questioned about their expectations on Baxter's behaviour, before applying the learned action model to a new scenario. Users were asked open-ended questions (``What will Baxter do when applying the learned action model?"), so that their responses were unbiased. When they encountered failure scenarios, they were asked to reason about Baxter's behaviour.} 
\end{itemize}

 \begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/eEvaluation}
  \caption{Summary of questionnaire responses: Extract of 18 questions on the user's understanding, after the experiment to teach action models by demonstration (Sec. \ref{sec:Exp2}).}
  \label{fig:eEvaluation}
\end{figure}


\subsection{Implementation details - Wizard of Oz}
\subsubsection{The User Interface}
Our user interface was built upon the existing ROS package called \textsc{Baxter Tasker} created by Generation Robots (\cite{BaxterTasker}).
\textsc{Baxter Tasker} is intended for the Baxter Research Robot and provides a user interface on both the computer and the robot.
The user interface was initially designed to allow users to set up a simple pick and place behaviour and provides a general overview of Baxter's capabilities.
The Python classes combine the use of sensor data and existing Baxter interface classes.
\fig{fig:interface} shows the added functionalities.
In our implementation, we did not focus on the interface layout, hence did not modify the existing one.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{figures/interface}
	\caption{Implemented Baxter interface for Programming by Demonstration}
	\label{fig:interface}
\end{figure}


%The package includes the following functions which were particularly useful for this project:
%\begin{itemize}
%\item Conversion of pixel coordinate to limb x-y-position
%\item
%\end{itemize}



\subsubsection{Perceptions}
During the demonstration, our framework makes use of the sensors provided by the Baxter Research Robot.

As Baxter was designed for easy arm manipulation, we use teleoperation (\sect{subsec:Gathering demonstrations}) for demonstrations.
This is the most accurate way to transfer the teacher's intention to Baxter and when performing the task as the teacher handles Baxter's limb directly when demonstrating the task.
We initially considered the use of the electric gripper, but realised that it was more appropriate for precision tasks, as the gripper only allows a limited grasping area.
Suction grippers are more commonly used in manufacturing environments for the manipulation of objects with arbitrary size.
Thus, in our implementation we only use Baxter's arm with a suction gripper.
It is assumed that the searched objects have a flat surface for the vacuum gripper to grasp.

Baxter is equipped with a camera on the screen and one on each end effector.
The head camera is not very flexible, as the screen does not tilt enough.
For state observation and object localisation, we decided to only use the camera on Baxter's end effector, that is used for the manipulation task.

%\subsection{Knowledge representation} 
%There has been little research addressing the sharing and reuse of knowledge.
%OpenCyc (\cite{lenat1990cyc})
%OpenRobots Ontology (ORO) (\cite{lemaignan2010oro}) creates an abstract representation for high-level knowledge that can be shared among multiple platforms.
%In \cite{lemaignan2017artificial} it is loaded as a priori knowledge and used as an active knowledge base to manage symbolic facts and beliefs produced and shared by cognitive modules such as language processing, task planning, or geometric reasoning modules.
%The authors gather knowledge from three sources: a priori knowledge, knowledge acquired at run-time from perception, and symbolic statements from inferences produced by the reasoner.
%SPAtial Reasoning\&Knowledge (SPARK) (\cite{sisbot2011situation}) is a geometric and temporal reasoning module that can produce symbolic statements to describe the state of the robot environment and its evolution over time, e.g. <BOOK1 type Book, BOOK1 isOn TABLE>, \\
%find(BOOK2 isAt ?location) \\
%find(?obj type Book, ?obj differentFrom BOOK1)\\
%TAPE isVisible true \\
%TAPE isReachable true\\ 
%TAPE isOn TABLE\\
%TAPE isIn BIN\\
%ROBOT hasInHand TAPE\\
%\cite{lemaignan2017artificial} use 2D fiducial markers for object recognition and have about 10 markers present during the experiment at a time.
%RoboEarth (\cite{waibel2011roboearth}) collects, stores, and shares data independent of the robot hardware to allow reuse of data for robotic platforms.
%Other approaches include the CRAM/KnowRob architecture (\cite{bibid}) the knowledge base is an active hub that pro-actively queries perceptual components to acquire knowledge.

%We defined a set of types and predicates for our domain model 


%%%% Step 2 Learning %%%%
\subsubsection{Learning}
%%%% Model of the skills %%%%
%\subsection{Create an action model}
\label{Create an action model}
Recall the action models in PDDL as part of the \texttt{PRODUCTION} domain mentioned in \sect{subsec:Planning domain description}.
In order to teach Baxter a complete pick-and-place action, we need to teach Baxter all action models (pick, move, drop) to solve the \texttt{PERMUTATION} planning problem.
It is sometimes necessary to teach the robot all basic component actions separately, as they all have varying preconditions and effects.
The user should be able to create each of these action models by demonstrating the action and letting Baxter derive the associated conditions.
%This includes opening and closing the gripper, as well as the limb movement itself.


Thus, a simple pick-up action should be demonstrated as approaching the object, closing the gripper and lifting the object vertically.
An example of the observed action model of a \texttt{pick} action with the Baxter's right limb, grasping a red-coloured object at a starting position $(0,0)$ is the following:

\begin{verbatim}
(:action pick
:parameters (red - product 0-0 - location right - gripper)
:precondition  (and  (at red 0-0) (at-gripper right 0-0) 
(free right) not(empty 0-0))
:effect (and (carry red right)
(empty 0-0)
(not (at red 0-0)) 
(not (free right))))
\end{verbatim}
In order to execute the action, the robot observed that the red-coloured product was at location 0-0 and that the right gripper was free.
The effect of the \texttt{pick} action was that the right gripper was carrying the red product and that it was no longer occupying position 0-0.
Instead of replicating the demonstrated movement, the robot extracts the relevant characteristics of the gesture that need to be reproduced.
%To learn the action, it needs to observe the main attributes that can change and that are relevant to reproduce the action.
The world before and after each action is observed and the change is then recorded.


The above action model shows the learned \texttt{pick} action with the observed preconditions and effects, before and after the action execution.

The robot learns a generalised action model by removing the instantiated values for the variables \texttt{product}, \texttt{position} and \texttt{gripper}:

\begin{verbatim}
(:action pick
:parameters (?p - product ?l - location ?g - gripper)
:precondition  (and  (at ?p ?l) (at-gripper ?g ?l) (free ?g))
:effect (and (carry ?p ?g)
(not (at ?p ?l)) 
(not (free ?g))))
\end{verbatim}

\subsubsection{Implementation}
Due to the limited scope of this project, we only implemented the observed characteristics that were crucial to reproduce the action (see Table \ref{tab:PDDL implementation}).
For predicates describing the object's state, we would need to use a second camera, as the one on the end effector would have a restricted view after the grasping an object.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{l|l}
			PDDL & Implementation\\ \hline
			\texttt{:action pick} & action_name: String\\
			\texttt{at-gripper ?g ?l} & change_vector: Pose\\
			\texttt{free ?g } & gripper_change: Bool\\
			\texttt{carry ?p ?g} & holding_object: Bool
		\end{tabular}
		\label{tab:PDDL implementation}
		\caption{Implementation of the PDDL predicates}
	\end{center}
\end{table}

The \texttt{change_vector} is the trajectory calculated by taking the difference between the gripper's initial pose and goal pose.
This allows a learned movement to be reproduced from any arbitrary gripper position.
The \texttt{gripper_change} variable states whether or not the gripper status should be changed (between open and close) or whether it should be simply ignored.
The \texttt{holding_object} attribute (see Section \ref{holdingObject}) indicates whether the gripper is currently holding an object.

%\begin{table}[h]
%\begin{center}
%\begin{tabular}{l|l}
%Action name & Observed attribute changes\\ \hline
%Move horizontically & End effector's position\\
%Move vertically & End effector's position\\
%Rotate gripper pose & End effector's pose\\
%Grasp object & gripper status \\
%\end{tabular}
%\end{center}
%\label{tab:Action examples}
%\caption{Examples of possible actions to be learned}
%\end{table}

Our implementation only considers one demonstration for the learning process.
Different learning algorithms (e.g.
Bayesian, neural networks) might be required to improve the performance of the learned trajectories by taking into consideration multiple demonstrations.

Furthermore, in order to allow planning for more complex control structures in real-world domains, it is necessary to detect divergences of the planned states from actual sensed states.
The use of multiple cameras would be required to verify the actual states.
%\cite{ObjectActionComplexes} Another possibility is to learn a visual object model of an unknown object by grasping it to allow grasping different kinds of objects.

%\subsubsection{Learning preconditions and effects}
After the demonstration of the action, the user needs to verify the observed conditions associated with the action.
Due to the limited time and scope of this project, we did not implement the translation of the internal action model into PDDL.
However, the user can select the predicates to be associated with the action via the user interface.
The user interface proposes all predicates provided in the domain description and generates the PDDL code according to the user's selections.
In future work this could be automated, such that these predicates are automatically recognised by the robot from the taught action.

\subsubsection{Reproduction}
After the creation of all action models, the user should be able to reproduce a task as a combination of all actions, in order to validate its usability.
We consider the pick-and-place action of an object as a sequence consisting of four subtasks:
\begin{enumerate}
	\item Reach the grasp position: align the gripper vertically with the object, i.e.
	\texttt{search}
	\item Grasp the object: approach the object vertically and close the gripper, i.e.
	\texttt{pick}
	\item Move the object: perform the learned limb movement to the goal position while holding the object, i.e.
	\texttt{move}
	\item Place the object on its goal position: open the gripper to release the grasped object, i.e.
	\texttt{drop}
\end{enumerate}

\subsubsection{Reach the grasp position}\label{Object Search}
The skill of searching an object will have to be an inbuilt functionality, which can't be learned by demonstration.
Possible extensions are to learn the shape and colour of new objects to be searched.
We considered the possibility of using QR codes to identify objects, but realised that an RGB colour search was more efficient and intuitive for our experimentations.
Furthermore, we considered tracking the object on-line using the camera.
As objects in our environment are static, this would be unnecessary, so we decided against it.
Thus, the search is performed on a picture that is regularly taken by the camera after a position change.

We used OpenCV, as there already exists a library that allows simple edge detection and object recognition from pictures.

At the beginning of the search, the camera is moved to a position where the entire working area can be captured in the image.
Using the OpenCV library (\cite{opencv}), we convert this image to a bit array and filter by the searched colour to track the location of an object.

Once the greatest area containing the searched colour has been identified, the grasp position is calculated by taking its center (see \fig{fig:detectObject1}).

We then convert the pixel location in the camera image to the x-y-coordinate of Baxter's end effector position and move the end effector by the position offset.
We iterate through this process by taking a picture with the updated camera position (see \fig{fig:detectObject2}) and calculate the distance to the object.
By repeatedly moving the camera towards the center of the object, we aim to align the center of the camera image with the center of the object.
The algorithm takes into account Baxter's positional accuracy of the joints ($\pm 5$ mm), hence stops when it has been sufficiently aligned.
\begin{figure}[h]
	\centering
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[scale=0.35]{figures/detectObject1}
		\caption{First iteration object detection}
		\label{fig:detectObject1}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.45]{figures/detectObject2}
		\caption{Second iteration object detection}
		\label{fig:detectObject2}
	\end{minipage}
\end{figure}

The simplified pseudo code for the search algorithm is as follows:

\begin{verbatim}
recognise entire working area
while error rate > error tolerance
error = distance of camera center to object center
position offset = distance of object center to actual image center
convert position offset to limb x-y-coordinates
move arm closer to object by converted offset
move arm by camera to gripper offset
\end{verbatim}
Aligning the end effector with the object center is particularly important, as the suction gripper can only grasp the object, if gripper is entirely placed on the homogenous surface.
Grasping the object at the edge can lead to the object being dropped during the execution of the limb movement.

\subsubsection{Grasp the object}
After the gripper has been vertically aligned with the object, the learned \texttt{pick} action should grasp the object.
In the general case, the \texttt{pick} action should be learned by demonstration, as different robots have different grippers.
However, for the purposes of conducting experiments to evaluate our framework, we implemented an approach function, which replaces the \texttt{pick} action.
Depending on the assumptions made upon the objects in the domain and considering Baxter's sensor limitations, we considered several implementations of this function:
\begin{itemize}
	\item Fixed height approach: All objects in this domain have the same height, hence the distance between end effector and object is constant.
	The vertical approach can be fixed to a constant movement in the implementation code.
	\item Flexible object height: Objects have different sizes, hence the distance to the object is not constant.
	The vertical approach uses the end effector's range sensor and approaches the object until it detects the object in a close distance.
\end{itemize}
We implemented an approach function to deal with varying object heights using Baxter's infrared range sensor.
However, range sensors can return unreliable values, depending on the surface of the object (matte or glossy).
Moreover, the placement of the end effector at the edge of an object -- e.g.
for objects with small surfaces -- can skew the returned values.
One possibility to address this problem is to verify that the end effector is still approaching the object and not remaining static.


Depending on the robot's capabilities, there are many other potential implementations for this function.
For example, the user interface could provide an option to specify the object's height, or if the range sensor is reliable enough, then the object distance could be observed as part of the learning process.
For our experiments, we assumed that all objects in the domain were of the same height and, therefore, used the fixed height approach.


\subsubsection{Move the object}\label{holdingObject}
The move action is learned by demonstration and executed using the learned action model described in Section \ref{Create an action model}.
However, during the action execution, we need to make sure that the gripper is grasping the object.
Previously mentioned limitations (e.g.
due to the implementation of the approach function) or other unforseen problems can influence a successful grasp of the object.
As we experienced during our implementation tests, the surface of the object has to be entirely homogeneous, otherwise the object will be dropped after it has been lifted.
Thus, it is crucial to keep track of the gripper, i.e.
whether it is still holding the object during the execution of the subsequent actions.
This can be associated with the \texttt{carry ?p - product ?g gripper} predicate.
In this respect, we created a function \texttt{holding_object}, which returns the correct state, depending on the infrared range sensor (gripper is within a certain distance to the object) and the gripper status (suction is activated).

%%%% Step 3 %%%%
\subsubsection{Execution to a new context}
Combining all four subtasks allows us to reproduce the learned action in a new context.
The user interface offers the possibility to reproduce the learned actions in any arbitrary order.
This allows the user to verify whether the learned actions correspond to the demonstrated behaviour or not, and how the actions perform in different contexts.

\subsubsection{Goal-oriented programming}
Recall that our final objective is to teach Baxter a goal-oriented behaviour using automated planning techniques.
The user has completed the domain description with the created action models.
Given a domain, an initial state and a goal state, the automated planner will generate an action sequence that allows an efficient traversal from the initial state to the goal state.

To recognise the initial state of the world, we implemented a simple object localisation function using \texttt{object search} (see Section \ref{Object Search}), which saves the positions of the objects on the table.
Connecting the Baxter interface with an automated planner is a separate task, which we did not manage to implement in the restricted time of this project.
However, in our final implementation, Baxter can execute any sequence of actions listed in an external text document.
This allowed us to simulate an execution of a plan generated by an automated planner.

Note that in our implementation, we rely on a general assumption on the objects included in the domain.
The object search function assumes that all objects have a monotonous colour and a flat and homogeneous surface that guarantee a successful execution of the implemented \texttt{approach} and \texttt{holding_object} functions.

%In Section \ref{Conclusion} we will discuss possible extensions and respective implementations which allow the manipulation of other object types.


%Since we assume that there is only one gripper, the \textbf{gripperFree()} attribute does not take any arguments.
%For this simplified planning domain, we restrict the attributes to allow only objects of type block.
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{l|l|l|l}
%Action & Precondition & Effect\\ \hline
%move\_block(X,Y) & block\_at\_position(X,Y) and clear(Z) & block\_at\_position(X,Z) and $X \neq Z$ \\
%grasp(X) & gripper\_free() & not(gripper\_free())\\
%\end{tabular}
%\end{center}
%\label{tab:actionmodels}
%\caption{Action models }
%\end{table}%

%%%% Step 4 %%%%
\subsubsection{Retro-active loop for incremental learning}
After the new action has been created and tested, it is likely that the user needs to modify or improve them.
The execution to a new context provides the user with the possibility to test the selected predicates associated with the action.
The user can take advantage of the robot's incremental learning process.
This means the repeated use of action demonstrations by the teacher and the refinement of the action model.
Multiple demonstrations can be used to extract conditions that are commonly observed and learned trajectories can be refined by using statistical modeling techniques (\cite{ude1993trajectory}).
Hidden Markov Models (HMMs) can be used to encode temporal and spatial variations or to reproduce various motions (\cite{tso1996hidden}).
%\section{Generate and save the PDDL code for the learned action}


\subsection{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
All (11) users were satisfied with the PbD process and Baxter's abilities to learn and reproduce the demonstrated move action. All users understood the learned action model and managed to adopt the notion of preconditions and effects easily. As expected, no users pointed out the missing conditions (Fig. \ref{fig:scenarios-exp2}c), when asked for improvements of the initial action model (Fig. \ref{fig:scenarios-exp2}a).
Even users who were `experts' and who had heard of automated planning before, did not manage to create a complete action model from the start. However, all users detected the missing condition easily, when faced with the relevant failure scenarios.

In the final phase, users with no experience in automated planning (8) did not expect Baxter to solve the permutation problem, and agreed unanimously that it acted in an intelligent manner, when it did. At the end of the experiment, all users believed that they had taught Baxter a new task and the majority understood the representation of the action models well. No users encountered any difficulties during the experiment. 8 participants believed that no programming experience was required to teach Baxter a task (Fig. \ref{fig:eEvaluation}).

