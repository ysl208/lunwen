\select@language {english}
\addvspace {10\p@ }
\contentsline {xchapter}{Introduction}{5}{chapter.1}
\contentsline {figure}{\numberline {1.1}{\ignorespaces PbD Overview\relax }}{6}{figure.caption.4}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Tower of Hanoi problem a) with three disks b) with four disks\relax }}{7}{figure.caption.5}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Planning domain describing an initial world state, goal state, and a generated action sequence (left), where actions are described in terms of preconditions and effects (right).\relax }}{8}{figure.caption.6}
\addvspace {10\p@ }
\contentsline {xchapter}{Robot Programming}{11}{chapter.2}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Classical robot programming process\relax }}{12}{figure.caption.7}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Overview of Robot Programming methods.\relax }}{14}{figure.caption.8}
\contentsline {figure}{\numberline {2.3}{\ignorespaces KUKA Robotics graphical user interface (\cite {abdeetedal2017kuka})\relax }}{15}{figure.caption.9}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Lego Mindstorms EV3 icon-based interface (\cite {lego2003})\relax }}{16}{figure.caption.10}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Scratch: block-based visual programming language (\cite {majed2014learn})\relax }}{16}{figure.caption.11}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Types of Machine learning: Deep learning (supervised and unsupervised learning) and Reinforcement learning (\cite {jones2017models})\relax }}{19}{figure.caption.13}
\contentsline {figure}{\numberline {2.7}{\ignorespaces End-user involvement for common robot programming approaches\relax }}{21}{figure.caption.14}
\addvspace {10\p@ }
\contentsline {xchapter}{Automated Planning}{23}{chapter.3}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Definition of a planning problem: Left: (a) properties describing the initial world state (b) object names and their types (c) instantiated actions (d) properties describing the goal state. Right: Action model representation of a move action in terms of preconditions and effects: demonstrated action model for a cube (top), and generalised action model for any object, variables are prefixed with `?' (bottom).\relax }}{27}{figure.caption.16}
\addvspace {10\p@ }
\contentsline {xchapter}{A Robot Programming Framework for End-users in Cobotic Environments}{31}{chapter.4}
\contentsline {figure}{\numberline {4.1}{\ignorespaces A robot programming framework for non-expert users: the user teaches action models, which are used by the robot with an automated planner, to generate an action sequence to achieve a goal. After an unsuccessful robot execution, the user can refine the taught action models (dotted lines indicate user actions, solid lines indicate robot actions).\relax }}{32}{figure.caption.17}
\addvspace {10\p@ }
\contentsline {xchapter}{Pre-Experiments}{34}{chapter.5}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Experimental setup for user studies on Programming by Demonstration of a move action by kinesthetic manipulation of the Baxter robot.\relax }}{35}{figure.caption.18}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Users were instructed to provide a description of (a) the initial state of the world and (b) an initial move action model They derived additional preconditions for moving the ball from position A1 to B2: (c) \textit {(stackable ball cube)}: the ball can be stacked onto the cube, and (d) \textit {(empty B2)}: if the ball cannot be stacked, the target position should be empty.\relax }}{36}{figure.caption.19}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Definition of a planning problem (a) properties describing the initial world state (b) object names and their types (c) instantiated actions (d) properties describing the goal state.\relax }}{37}{figure.caption.20}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Summary of questionnaire responses: Extract of the 26 questions on the user's understanding, after the introduction to the automated planning language (Sec.~\ref {sec:Exp1}).\relax }}{37}{figure.caption.21}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Continuous refinement of the move action model: (a) initial action model learned by demonstration, (b) action model for all cubes of any colour, (c) action model with an additional condition, if the target position is occupied and cubes can not be stacked.\relax }}{38}{figure.caption.22}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Summary of questionnaire responses: Extract of 18 questions on the user's understanding, after the experiment to teach action models by demonstration (Sec. \ref {sec:Exp2}).\relax }}{39}{figure.caption.23}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Implemented Baxter interface for Programming by Demonstration\relax }}{40}{figure.caption.24}
\contentsline {figure}{\numberline {5.8}{\ignorespaces First iteration object detection\relax }}{43}{figure.caption.26}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Second iteration object detection\relax }}{43}{figure.caption.26}
\addvspace {10\p@ }
\contentsline {xchapter}{Simultaneous End-User Programming of Goals and Actions for Robotic Shelf Organisation}{47}{chapter.6}
\contentsline {figure}{\numberline {6.1}{\ignorespaces Overview of the developed system that allows users to demonstrate part of a shelf arrangement task and interact with a GUI to simultaneously program both the complete task goal (fully specified shelf arrangement) and the actions for achieving that goal.\relax }}{48}{figure.caption.27}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Main product shape categories, that comprise 90.1\% of items in the dataset, are defined based on the the shape of their base and their tip. The shape of the tip determines whether the objects can be stacked or not. The shape of the base determines whether the object is more compactly arranged on a regular grid (rectangular) or an off-grid arrangement (circular).\relax }}{50}{figure.caption.28}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Cylindrical item arranged in (a) regular grid and (b) off-grid configurations. (c) Soft-packaged item arranged in interleaved grid configuration.\relax }}{50}{figure.caption.29}
\contentsline {figure}{\numberline {6.4}{\ignorespaces The extended Rapid PbD interface with the inferred object arrangement (purple) visualized overlaying the detected objects (green).\relax }}{54}{figure.caption.31}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Learning curve of 10 teaching strategies for 4 different shelf arrangements from the Freiburg dataset, with number of actions (x-axis) and ranking of the ground truth shelf arrangement (y-axis).\relax }}{55}{figure.caption.32}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Average number of steps required to infer the correct arrangement per teaching strategy across the dataset (error bars show standard deviation).\relax }}{55}{figure.caption.33}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Simplified user-interface created to evaluate our goal inference model in an online user study. The top part shows the desired configuration with a picture and the visualization of the current shelf arrangement inference. The bottom part has three parts corresponding to three types of programming actions the user can take: demonstration (drag-and-drop items onto shelf), specification (select parameter values from drop-down menus), and selection (choose one of the top four most likely arrangements by clicking on it).\relax }}{56}{figure.caption.34}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Average number of steps required for 8 arrangement tasks chosen for the user study, by teaching strategy and human performance (AMT).\relax }}{57}{figure.caption.35}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Distribution of user strategies employed in the AMT study\relax }}{58}{figure.caption.36}
\contentsline {figure}{\numberline {6.10}{\ignorespaces Snapshots from the executions of the eight system evaluation benchmark tasks (Table~\ref {table:task\@uscore .list}).\relax }}{59}{figure.caption.38}
\addvspace {10\p@ }
\contentsline {xchapter}{Implementation of the Framework}{60}{chapter.7}
\contentsline {figure}{\numberline {7.1}{\ignorespaces PbD principle overview\relax }}{61}{figure.caption.39}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Overview of iRoPro that allows users to teach low- and high-level actions by demonstration. The user interacts with the GUI to run the demonstration, modify inferred action conditions, create new planning problems for the robot to solve and execute.\relax }}{62}{figure.caption.40}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Example of a high-level action for moving an object from A to B. Conditions are inferred from the observed predicates before ($O\@uscore .1$) and after ($O\@uscore .2$) the demonstration. \relax }}{63}{figure.caption.41}
\contentsline {figure}{\numberline {7.4}{\ignorespaces The iRoPro interface showing the action condition menu and an interactive visualisation of the Baxter robot and detected objects.\relax }}{66}{figure.caption.42}
\addvspace {10\p@ }
\contentsline {xchapter}{Experimentation and Evaluation}{68}{chapter.8}
\contentsline {figure}{\numberline {8.1}{\ignorespaces Snapshots from the executions of the system evaluation (Tasks 3\&4) showing a claw grip from top and side.\relax }}{69}{figure.caption.44}
\contentsline {figure}{\numberline {8.2}{\ignorespaces Experimental setup for the user study (N=21), where users programmed the Baxter robot via a GUI to manipulate given object types (with predefined type hierarchy). \relax }}{71}{figure.caption.45}
\contentsline {figure}{\numberline {8.3}{\ignorespaces Participants who did better in the pre-test questionnaire completed the main tasks faster, with `non-CS' users scoring the highest and being the fastest on average.\relax }}{73}{figure.caption.47}
\addvspace {10\p@ }
\contentsline {xchapter}{Conclusion and Future Work}{75}{chapter.9}
