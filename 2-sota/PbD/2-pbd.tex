\section{Programming by Demonstration}
PbD can be classified into low-level learning (trajectory encoding) and high-level (symbolic encoding) learning.
 Low-level representations focus on learning generalised trajectories from the human demonstrator, high-level representations focus on learning a sequence of motion elements (primitives) \cite{peppoloni2014ros}.

Our focus is on the former, learning low-level trajectories that can be generalised.
 The latter will then be taken over by automated planning techniques.

``Programming by demonstration (PbD) has become a central topic of robotics and spans across general research areas such as human-robot interaction, machine learning, machine vision and motor control." (\cite{billard2008robot})
It provides an intuitive medium to allow non-robotics-experts, who do not possess the necessary domain knowledge, to communicate skills to robots more easily.
 The underlying concept is to learn a new skill from a set of correct demonstrations.
It offers a framework for service robotics applications, independent of the robot platform, and reduces the overhead of reprogramming the robot for different tasks.

The problem of learning a skill can be considered as learning a function that maps a world state to an action.
 In real-world applications, states can only be partially observed due to restricted sensor availability.
 Hence, we assume that the robot learner has access to the observed state $Z$ instead.
 A function $\pi : Z \rightarrow A$, referred to as a \textit{policy}, allows the robot to select actions from an action domain $A$, given observations of the world state.
 
Programming by demonstration is an approach to learn this policy from demonstration data, known as \textit{examples}.
% ``Examples are sequences of state-action pairs that are recorded during the teacher's demonstration of the desired robot behaviour." \cite{argall2009survey} 
Unlike learning techniques in reinforcement learning, PbD is aimed at learning from correct examples only.
 Thus, PbD can be seen as a subset of Supervised Learning, as the agent is presented with labeled training data and tries to learn an approximation of the function, which produced the data (\cite{argall2009survey}).

 \cite{nicolescu2003natural} present a PbD approach, which allows the robot to learn skill representations and refine them by using feedback cues provided by the teacher.
 Similarly, \cite{calinon2007active} and \cite{calinon2007incremental} implement systems, which actively involve the teacher in the robot's learning process, by providing human guidance to a humanoid robot.
 The robot first observes the demonstration performed by the teacher, who is wearing motion sensors.
 When it tries to reproduce the action, the teacher can refine the movement by physically moving its limbs.

\subsection{Design choices}
There are different design choices to consider when developing a new PbD system.
We differentiate between the choice of the demonstrator (human or robot), in particular who controls the demonstration and who executes the demonstration.
 Consider the example of a simple pick up action of an object.
 The teacher could be demonstrating the action using its own hands, while the robot observes the motion, or by moving the robot's joints directly.
 Furthermore, the state and action can either be represented in a discrete way (\textit{``object on the table"} or \textit{``robot holding object"}) or in a continuous way using the 3D positions of the robot's end effector and the box.
 There is also the choice of the demonstration technique, i.e. whether the data is provided to the learner incrementally or once all data has been gathered.
 
Despite the different design choices, there are common aspects across all PbD applications. Firstly, the teacher demonstrates the desired behaviour and, secondly, the learner derives a policy from this set of demonstrations to reproduce the behaviour.

%Building the data set
\subsection{Gathering demonstrations} \label{subsec:Gathering demonstrations}
There are many ways for the robot to collect data.
 We differentiate between:
\begin{itemize}
    \item How much data is collected at a time: incremental vs batch learning
    \item Who provides the data: robot can look for data on its own or it is provided data by user
    \item Which interaction modalities are used: graphic/visual, voice, and kinesthetic. 
\end{itemize}

In contrast to reinforcement learning techniques, where the policy is learned from arbitrarily positive and negative experience, PbD approaches derive the policy from a dataset of supervised examples.
In PbD systems the teacher provides data for the robot to learn from.
This means that the selected data is 'biased'. 
The user can choose positive or negative examples (\cite{grollman2012robot}), to allow robot learn more efficiently.
Positive examples are demonstration data showing what the robot should do and what it can learn directly from, i.e. where an action changes predicates.
Negative examples are what the robot should avoid but should help it to generalise faster, i.e. when an action fails because its preconditions are not satisfied.
However, \cite{walsh2010efficient} noted his Ph.D. thesis that negative examples are highly uninformative as the learner cannot determine the reason for the failure, while positive examples provide very useful information because a superset of the literals that belong to the precondition is identified.

There exist various techniques for gathering demonstration examples.
 We differentiate between methods for action recording and action execution.
 Consider the two mappings (\cite{argall2009survey}):
\begin{itemize}
\item Record mapping $g_R(z,a)$: A mapping between the exact actions demonstrated by the teacher and those recorded within the dataset.
\item Embodiment mapping $g_E(z,a)$: A mapping between the actions recorded in the dataset and the actions executed by the robot learner.
\end{itemize}
If the teacher manipulates the robot's arm to demonstrate the desired behaviour and the joint trajectories are recorded, then the record mapping is the identity function ($g_R(z,a) = I(z,a)$).
 Otherwise, the teacher information is encoded and recorded into the dataset according to a mapping function ($g_R(z,a) \neq I(z,a))$.
 Take as an example a human teacher, who demonstrates the action of moving an object on the table using their own hand.
 The robot records the movement using its camera and tries to extract the necessary information to perform this action, while the teacher's actual joint movements remain unknown.
 Thus, the recorded dataset is extracted from the camera images using the record mapping.

Similarly, for the action execution, if the recorded actions in the dataset are executed directly by the robot learner, the embodiment mapping is the identity function ($g_E(z,a) = I(z,a)$).
 Otherwise, it is a function $g_E(z,a) \neq I(z,a)$, which translates the recorded data for the robot learner.
  We refer to the two different action execution methods as \textit{Demonstration} and \textit{Imitation} respectively.
When dealing with real robots, the embodiment mapping is particularly important, as the teacher's demonstration is executed in a physical environment, rather than a simulated one.
 Successful learning of the action will depend on an accurate mapping between the recorded dataset and the learner's movement capabilities.
  \begin{figure}[!h]
    \centering
    \includegraphics[scale=0.5]{figures/Gathering}
    \caption{Intersection of the record and embodiment mappings.}
    \label{fig:Gathering demonstrations}
  \end{figure}

Given the above mappings and categorisations, we differentiate between four data acquisition approaches, depending on whether the record or embodiment mappings are equal to the identity function or not (see also Figure \ref{fig:Gathering demonstrations}):

\begin{itemize}
\item Teleoperation ($g_R(z,a) = I(z,a)$ and $g_E(z,a) = I(z,a)$): The teacher operates the robot learner platform, while the robot's sensors record the task execution of its own joints.
\item Shadowing ($g_R(z,a) \neq I(z,a)$ and $g_E(z,a) = I(z,a)$): The teacher demonstrates tasks using their own hands, while the robot learner records the execution and tries to mimic the teacher's task execution.
\item Sensors on teacher  ($g_R(z,a) = I(z,a)$ and $g_E(z,a) \neq I(z,a)$): The teacher wears sensors, which record the task execution.
 The robot learner needs to translate the teacher's joint positions for its own task execution.
\item External observation  ($g_R(z,a) \neq I(z,a)$ and $g_E(z,a) \neq I(z,a)$): The teacher executes the tasks using their own hands, while external sensors, that may or may not be on the robot learner, are used to record the teacher's task execution.
\end{itemize}

A full categorisation for building the demonstration dataset is shown in Figure \ref{fig:Gathering demonstrations2}.
 The use of intermediate mappings can lead to a loss of information of the demonstrated actions.
 Teleoperation is the most accurate way to transfer the teacher's intention to the robot, as the exact actions are recorded and executed without any intermediate translations.
 The choice for gathering demonstration data depends highly on the robot's sensory capabilities and the complexity of the demonstrated task.
 If the robot's joints do not correspond to those of the teacher, it is difficult to find an accurate mapping.
 For the implementation of our framework, we decided to take a teleoperation approach.
 The teacher will be handling the robot's limb directly, when demonstrating the task, while the robot records its own joint movements.

  \begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{figures/Gathering3}
    \caption{Typical approaches to providing demonstrations (\cite{argall2009survey})}
    \label{fig:Gathering demonstrations2}
  \end{figure}


\subsection{Challenges in PbD}\label{sssec:Challenges in PbD}

\paragraph{Suboptimality of demonstrations.}
Another known PbD problem exists with regards to the type and quality of the demonstration which is dependent on the teacher's knowledge of the robot's system.
Na\"{\i}ve teachers may have greater assumptions on the robot's intelligence and take less care in executing demonstrations as compared to roboticists, who understand the effects of noisy demonstrations (\cite{suay2012practical}).
\cite{chen2003programing} and \cite{kaiser1995obtaining} recognised different sources for sub-optimality in demonstration such as the user demonstrating unnecessary or incorrect actions due the lack of knowledge about the task.
\cite{cakmak2014teaching} developed a PbD system and investigated the use of instructional materials in the form of tutorials and videos to support the learnability.

\paragraph{Lack of comparison between algorithms.}
Despite there being many PbD algorithms proposed in research literature (\cite{argall2009survey,billing2010formalism}), there remains a lack in comparative user studies.
Difficulties in comparing across applications arise from the use of different robotic platforms and demonstration techniques, which lead to different representations of demonstration data.
\cite{suay2012practical} partly addresses this challenge by comparing three well-established algorithms and evaluating their performance in a common domain.


\paragraph{Permutation example}
Consider the task of permutating two objects, which are red and blue and located at positions A and B respectively.
Our goal is to permutate the two objects, such that red is located at B and blue at A (see Figure \ref{fig:Permutation}).
The robot is only equipped with one arm and provided with three positions A, B and C.
As humans, we naturally understand that we need to empty one of the two occupied goal positions (A or B), by making use of the empty space C.
If the robot was only taught to move objects, but not the conditions for moving them (i.e.
 that the goal position needs to be vacant before placing an object), it will not be able to produce the same reasoning as us.
As existing PbD implementations are not goal-oriented, the robot needs to be taught the precise action sequence for the permutation of two objects.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{figures/PbD-permutation}
	\caption{Initial state and goal state for the permutation of two objects}
	\label{fig:Permutation}
\end{figure}

The act of placing an object from one position to another needs to be associated with the condition of having an empty goal position prior to action execution.
In other words, we set a \textit{precondition} (\textit{``goal position is empty"}) on the state of the world, which needs to be fulfilled for this action to be executed.
Similarly, we can set conditions for the state of the world \textit{after} the action execution, known as \textit{effects}, e.g.
 \textit{``object is at goal position"}.
Thus, an \textit{action model} for the pick-and-place action consists of the following conditions:

\begin{table}[h]
	\begin{center}
		\begin{tabular}{l|l}
			Preconditions & Effects\\ \hline
			& \\
			The object X is at the initial position A.
			& The object X is at the goal position B.\\
			The goal position B is empty.
			& The object X is not at the initial position A.
		\end{tabular}
	\end{center}
	\label{tab:conditions}
	%\caption{Preconditions an}
\end{table}

This representation, in terms of preconditions and effects, is used in Automated Planning techniques.
It allows the robot to reason about its tasks by keeping track of the state of the world after each action execution.
Using these action models, automated planners can generate an action sequence to achieve a goal.
However, automated planners rely on a classical planning language called PDDL, which represents the conditions in first-order logic (see \fig{fig:Pick-up action model}).
%Action models, with conditions in first-order logic (see Figure \ref{fig:Pick-up action model}), are used to generate sequences of actions, known as \textit{plans}, to achieve a goal.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{figures/schema-logic}
	\caption{Action model representation in first-order logic}
	\label{fig:Pick-up action model}
\end{figure}
Recall the example given earlier to permutate two objects (see Figure \ref{fig:Permutation}).
Given an initial state (object red at position A, object blue at position B) and a goal state (object red at position B, object blue at position A), the planner can automatically generate an action sequence, which, if executed successfully, transitions from the initial state to the goal state: \texttt{move(blue,B), move(red,B), move(blue,A)} (see Figure \ref{fig:Automated Planner}).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{figures/PbD-AutomatedPlanner}
	\caption{Action sequence automatically generated by an automated planner}
	\label{fig:Automated Planner}
\end{figure}

\subsubsection{Learning by Watching/Observation}\label{sssec:LbObservation}
(\cite{kuniyoshi1994learning})
- teacher provides robot with data to learn from e.g.
 learn from watching videos (\cite{Yang2015})
but difficulty lies in providing the optimal \& minimal set of demonstration data
 